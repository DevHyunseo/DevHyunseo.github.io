---
title:  "선형회귀 (Linear Regression) : 가장 기본적인 머신러닝 모델 이해하기"
date:   2024-10-01 21:03:36 +09:00
categories: [AI, 머신러닝]
tags:
    [
        선형회귀,
        AI,
        머신러닝,
        지도학습
    ]
use_math: true 
published: true
---

지도 학습 모델(Supervised Learning Model)은 머신러닝의 한 종류로, 입력 데이터(Input)와 이에 해당하는 정답 데이터(Output, 레이블)를 사용하여 모델을 학습시켜 새로운 데이터에 대해 예측을 수행하는 모델이다.

지도 학습은 하나 이상의 입력을 하나 이상의 출력으로 매핑하는 "일종의 방정식"이라고도 할 수 있다. 간단한 선형회귀 모델을 통해서 이를 알아보자.

선형회귀는 통계학과 머신러닝에서 가장 기본적이면서도 중요한 모델로, 독립변수(설명 변수)와 종속 변수(반응 변수) 간의 선형 관계를 모델링하여, 주어진 입력에 대한 출력을 예측하는 데 사용된다.

---

선형회귀(Linear Regression)
----
선형회귀는 주어진 데이터를 기반으로 선형 직선을 만들어 입력 변수와 출력 변수 간의 관계를 가장 잘 설명하는 직선을 찾는 것이다. 결국 직선의 방정식 $f(x) = mx + b$이 있을 때 __입력 데이터를 가장 잘 설명하는 직선의 기울기 $m$와 절편 $b$를 찾는 문제__ 라 할 수 있다. 

![image](https://i.ibb.co/kHMXDzW/IMG-CB2-A9-D258554-1.jpg)
blue dots이 주어진 데이터일 때, 이 데이터를 "가장 잘 설명하는" 직선을 찾는 것이 우리의 목표이다. 기울기와 절편의 값에 따라 직선이 여러 개 나올 수 있는데 선형회귀는 데이터에 따른 여러 직선을 맞추어 본 후에 가장 잘 맞는 직선을 찾는 것이라 할 수 있다.  

선형회귀의 간단한 예로, 광고 지출(독립 변수)에 따른 판매량(종속 변수) 예측할 수 있고, 학생의 공부 시간에 따른 시험 점수 예측, 소득에 따른 소비 지출 분석도 할 수 있을 것이다.

<br>

단순 선형 회귀 (Simple Linear Regression)
----

#### 단순 선형 회귀 수식
직선 방정식$ f(x) = wx + b$ 를 단순 선형 회귀의 수식으로 표현하면 다음과 같다.

$$ y = \beta_0 + \beta_1 x + \epsilon $$

- $y$ : 종속 변수 (예측값)
- $x$ : 독립 변수 (입력값)
- $\beta_0$ : 절편 (Intercept) – 그래프가  y축과 만나는 지점
- $\beta_1$ : 회귀 계수 (Regression Coefficient) 또는 기울기 (Slope) – 독립 변수  x 가  y 에 미치는 영향
- $\epsilon$ : 오차 항 (Error Term) – 예측값과 실제값 간의 차이

단순 선형 회귀는 독립 변수 $x$ 하나를 사용하여 종속 변수 $y$ 를 예측하는 가장 기본적인 회귀 분석 방법이다. 데이터 간의 선형 관계를 수학적으로 모델링하여, 주어진 입력값에 대해 출력값을 예측하는 것이다.

선형 회귀 모델에서 예측값과 실제값 사이의 차이가 있을 것이다. 이 차이를 수학적으로 설명하기 위해 오차항 ( $\epsilon$ )이 추가된다. 오차항은 모델이 설명하지 못한 데이터의 불확실성, 숨겨진 변수, 또는 데이터의 노이즈를 반영하게 된다.

<br>

다중 선형 회귀(Multiple Linear Regression)
----
독립 변수 $x$가 1개 아니라 여러 개일 수도 있다. 그런 경우, 다중 선형 회귀의 수식으로 표현할 수 있다.

$$ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n + \epsilon $$

- $y$ : 종속 변수 (예측값)
- $x_1, x_2, \ldots, x_n$ : 독립 변수 (입력값들)
- $\beta_0$ : 절편 (Intercept) – 모든 독립 변수가 0일 때의 종속 변수 값
- $\beta_1, \beta_2, \ldots, \beta_n$ : 회귀 계수 (Regression Coefficients) 또는 가중치 (Weights)
- $\epsilon$ : 오차 항 (Error Term)

<br>

선형 회귀의 학습
----
선형 회귀의 목표는 "기울기와 절편을 추정"하여 $y$ 와  $x$  사이의 선형 관계를 가장 잘 설명하는 직선을 찾는 것이다. 기울기와 절편은 파라미터(parameter)라고도 부른다.

선형 회귀 모델을 __"학습(training)시킨다"는 것은 훈련 데이터에 가장 잘 맞도록 모델의 파라미터(기울기, 절편)를 찾아가는 것__ 이다. 처음에 랜덤하게 직선을 만든 다음, 학습을 반복해서 가장 잘 맞는(fit) 직선을 찾아나간다. 잘 맞는(fit) 직선이란 예측한 값과 실제 값의 오차가 가장 적은 직선이라고 할 수 있다. 이 내용은 손실함수(Loss Function)에서 알아보자.