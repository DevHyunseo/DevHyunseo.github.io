---
title:  "인공 신경망(Artificial Neural Network): 인공지능의 기본 구조"
date:   2024-10-02 10:03:36 +0530
categories: [AI, 딥러닝]
tags:
    [
        인공신경망,
        퍼셉트론,
        활성화함수,
        뉴런,
        딥러닝,
		    AI,
    ]
use_math: true 
published: true
layout: post
---

인공 신경망(Artificial Neural Network)은 인간의 뇌가 학습하는 방식에서 영감을 얻어 데이터를 처리하고 예측하는 딥러닝의 핵심 기술이다.
___

인공 신경망(Artificial Neural Network)
---

우리 생활에서는 선형적인 문제보다 비선형적인 복잡하고 어려운 패턴의 문제가 더 많다. 인공 신경망은 복잡한 문제를 해결할 수 있고, 다차원의 입출력 데이터도 응용할 수 있는 머신러닝 알고리즘이다.

### 뉴런(Neuron)
<center>
<img src="https://miro.medium.com/v2/resize:fit:2902/format:webp/1*hkYlTODpjJgo32DoCOWN5w.png" alt="Neural Network Architecture">
</center>

인공 신경망은 생물학적 신경망(인간의 뇌 구조)을 모방하여 설계된 알고리즘이다. 뇌의 신경세포인 뉴런은 수상돌기(Dendrite)라는 부분과 축색돌기(Axon)라는 부분으로 구성되어 있다. 수상돌기는 여러개의 입력 신호를 받는 역할을 하며 수상돌기에서 받은 모든 입력신호는 핵에서 처리된 후 하나의 출력이 되어 축색돌기를 통하여 다음 단계로 전달된다. 즉, 여러 개의 입력 신호를 받아서 조합한 후에 다음으로 출력 신호를 보낼지 말지 (1 or 0) 결정한다.

### 인공 신경망(Artificial Neural Network)

인공 신경망은 이러한 뉴런을 연결한 형태로 <u> 입력을 받아 각각에 가중치를 곱하고, 편향을 더한 뒤 활성화 함수를 통해 출력값을 생성</u>한다. 

입력 데이터 $x_i$와 각 입력에 곱해지는 가중치 $w_i$의 곱에 편향  $b$ 을 더하는 수식을 표현하면 다음과 같다.

$$z = \sum_{i=1}^n w_i x_i + b$$

- $x_i$: 각 입력 데이터.
- $w_i$: 가중치.
- $b$: 편향.

편향 $b$ 가 없다면 뉴런이 항상 원점을 지날 것이므로 수식에 편향을 넣어서 모델의 표현력을 높혔다.

가중치 $w_i$ 는 각 뉴런의 연결 강도를 나타내는 값이다. 입력 값은 각각의 가중치와 곱해진다. 따라서 <u>가중치가 클수록 특정 입력 값이 더 중요한 영향을 미친다</u>고 할 수 있다. 신경망이 학습할 때, 가중치는 경사하강법(Gradient Decent)와 같은 최적화 알고리즘을 통해 손실 함수(Loss Function)를 최소화하도록 업데이트되어 가중치가 점진적으로 조정된다. 이에 대한 내용은 손실 함수, 경사하강법을 다룰 때 자세히 알아보자.

이렇게 구한 $z$는 입력 값에 각각의 가중치를 곱한 값들을 모두 더한 것으로 가중치의 합 즉, 가중합(Weighted Sum)이라고도 불린다.

<center><img src="https://wikidocs.net/images/page/219430/KakaoTalk_20240114_195128227_07.png" width="600" height="600" alt="가중합"></center>

계산된 가중합 $ z$ 는 활성화 함수(activation function)로 전달한다. 활성화 함수는 신경망의 뉴런을 활성화해야 할지  여부를 결정하는 함수라고 할 수 있다.


### 활성화 함수(Activation Function)

활성화 함수는 인공 신경망의 중요한 부분으로 뉴런의 출력에 쓰이는 함수이다. 활성화 함수는 <u>뉴런이 활성화되어야 하는지 여부</u>를 결정한다고 하였다. 이는 더 간단히 말해 예측 과정에서 이 뉴런의 입력이 중요한지 여부를 결정할 것임을 의미하기도 한다. 
활성화 함수는 앞서 구한 가중합 $ z $에 <u>비선형성을 추가</u>하여 신경망이 단순히 선형 연산만 수행하는 것이 아니라 복잡한 패턴의 데이터를 학습할 수 있게 된다. 여기서 비선형성을 추가한다는 말을 직관적으로 이해해보자.

#### ReLU 함수

가장 많이 쓰이는 활성화 함수 ReLU를 살펴보자.

<center><img src="https://velog.velcdn.com/images/sckim0430/post/debca649-646e-41b7-9f56-0f7df431f8b4/image.png" alt="relu"></center>

$$ f(z) = \max(0, z) $$

ReLU 함수는 $ z $ 가 0보다 크면 $ z $ 를 그대로 출력하고 0보다 작다면 모두 0으로 만든다. 선형이었던 $ z $ 가 ReLU 함수를 통과하면서 $z$ 가 0인 지점에서 직선이 꺾이는 모양이 된다. 모든 활성화 함수는 이러한 "비선형적인 형태"를 가지고 있다. 활성화 함수가 포함된 층을 여러 개 쌓으면, 각 층의 비선형 형태가 누적되어 매우 복잡한 비선형 패턴을 학습할 수 있다. 이것이 DNN(Deep Neural Network)이다. 자세한 내용은 DNN을 다룰 때 알아보자.

이 밖에도 Softmax, Leakly ReLU, ReLU 등 다양한 활성화 함수가 있다. 활성화 함수는 Gradient Vanishing와 그 밖에 대한 설명도 필요하기 때문에  활성화 함수에 대해서는 따로 포스팅을 할 것이다.

퍼셉트론(Perceptron)
---
퍼셉트론은 1958년 프랭크 로젠블랫(Frank Rosenblatt)이 개발한 모델로, 단층 신경망(Single Layer Neural Network)로 이진 분류 문제를 해결하기 위해 설계된 알고리즘이다. 

<center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FCd30x%2FbtqG9Cjkx2p%2FAuAD6ThYgAkIt8kIctO7CK%2Fimg.png" alt="perceptron"></center>

$$z = \sum_{i=1}^n w_i x_i + b $$

앞서 가중합으로 구한 $z$를 <u>단위 계단 함수(Unit Step Function)</u>라는 활성화 함수를 사용하여 출력했다. 
(위 그림에서는 편향을 상수 1로 구현했다.)

<center><img src="https://www.researchgate.net/profile/Jan-Awrejcewicz/publication/302218545/figure/fig14/AS:402896718974983@1473069471684/A-unit-step-function-sign-t.png" alt="stepfun"></center>

$$f(z) =
\begin{cases}
1, & \text{if } z \geq 0 \\
0, & \text{if } z < 0
\end{cases}
$$

이러한 퍼셉트론으로 "이진 분류"가 가능하다. AND 연산을 예시로 들어보자. 
AND 연산은 두 입력값이 모두 1일 때만 출력이 1이고, 나머지는 0이다. 따라서 다음 조건을 만족한다.

$w_1x_1 + w_2x_2 + b \geq 0 \text{ (출력: 1)}$
$w_1x_1 + w_2x_2 + b < 0 \text{ (출력: 0)}$

만약 각 가중치와 편향을 다음과 같이 임의로 설정하고,
$w_1 = 1, w_2 = 1, b = -1.5$

가중합 $z$를 구하면 ,
$z = w_1x_1 + w_2x_2 + b$

$f(z) =
\begin{cases}
1, & \text{if } z \geq 0 \\
0, & \text{if } z < 0
\end{cases}
$

$x_1 = 0, x_2 = 0$ 일 때,
$z = 1 \cdot 0 + 1 \cdot 0 - 1.5 = -1.5$
$z < 0$ 이므로 $f(z) = 0$

$x_1 = 0, x_2 = 1$ 일 때,
$z = 1 \cdot 0 + 1 \cdot 1 - 1.5 = -0.5$
$z < 0$ 이므로 $f(z) = 0$

$ x_1 = 1, x_2 = 0$ 일 때,
$z = 1 \cdot 1 + 1 \cdot 0 - 1.5 = -0.5$
$z < 0$ 이므로 $f(z) = 0$

$ x_1 = 1, x_2 = 1$ 일 때,
$z = 1 \cdot 1 + 1 \cdot 1 - 1.5 = 0.5$
$z \geq 0$ 이므로 $f(z) = 1$

결과적으로 입력값이 (0, 0), (0, 1), (1, 0) 일 때는 출력값이 0이 되고 입력값이 (1, 1) 일 때는 출력값이 1이 되는 AND 연산이 가능함을 확인하였다.

<center><img src="https://wikidocs.net/images/page/24958/andgraphgate.PNG" alt="img"></center>

더 직관적으로 설명하면 출력 0과 1을 나눌 수 있게 적절하게 직선을 그으면 AND 연산을 표현할 수 있다. 어떤 dot이 들어와도 퍼셉트론이 만들어낸 이 직선을 기준으로 0인지 1인지 판가름할 수 있다.

그러나 퍼셉트론은 AND, OR 문제까지 가능하지만 XOR 문제는 해결할 수 없다. 하나의 직선을 어딘가에 그어서는 XOR 문제를 해결할 수 없다. 선형적이지 않기 때문이다. 
<center><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FkzozO%2FbtqA0OR0l7G%2FyuHw8Y762KYUfnaoP4Ymx1%2Fimg.png" alt="img2"></center>

1969년 마빈 민스키와 시모어 페퍼트가 이러한 퍼셉트론의 한계를 수학적으로 증명하여 이후 인공지능의 겨울이라 불리는 암흑기가 도래했다. 인공지능에 대해 회의적인 전망이 이어지고 인공지능 연구와 프로젝트에 대한 지원이 끊기게 되었다.

XOR 문제를 해결하려면 다층 신경망(Multi-Layer Neural Network)이 필요하다. Multi-Layer 즉, 레이어를 추가하여 해당 레이어에 비선형성을 추가해야 한다. 이에 대해서 다음 포스트에서 자세히 알아보자.