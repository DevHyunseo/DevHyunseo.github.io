---
title:  "활성화 함수(Activation Function): 종류와 장단점 "
date:   2024-11-06 10:03:36 +0530
categories: [AI, 딥러닝]
tags:
    [
        SNN,
        인공신경망,
        딥러닝,
		AI,
    ]
use_math: true 
published: true
---


활성화 함수(Activation Function)
---

#### Sigmoid 함수

<p align="center">
  <img src="https://mlnotebook.github.io/img/transferFunctions/sigmoid.png" alt="sigmoid">
</p>

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

시그모이드 함수는 출력값을 [0, 1]로 변환하는 함수이다. 이 함수는 신경망 초기에는 많이 쓰였지만 지금은 쓰이지 않는다. 그 이유는 크게 두 가지가 있다.

	•	Sigmoid 출력값이 0.5 근처에서 몰리면(Zero-Centered되지 않음), 모든 뉴런의 그래디언트가 작아지게 됩니다.
	•	이로 인해, Zig-Zag로 인한 방향성 문제와 더불어, 그래디언트 소실이 심화되어 가중치가 거의 업데이트되지 않을 수 있습니다.
__(1) zero-centered하지 않아서 수렴 속도가 느리다.__

시그모이드 함수의 출력은 $0 \leq \sigma(z) \leq 1$ 항상 양수라는 점이 문제가 된다. 결과값의 중심이 0이 아니라 0.5이다.(=zero-centered하지 않다.) 출력값이 항상 양수이기 때문에 역전파 과정에서 가중치의 그래디언트 업데이트 방향이 한쪽으로 편향되어 학습 과정에서 가중치가 좌우로 흔들리는 “Zig-Zag” 패턴을 가진다. 

간단히 설명해 보면, 인공신경망은 학습을 하면서 모델의 가중치를 업데이트해 나간다. 가중치를 얼만큼 업데이트할 지는 손실 함수를 계산하여 결정한다. 손실 함수는 간단히 말해 실제 값과 모델이 만든 예측 값 사이의 오차를 의미한다. 그 오차가 작을수록 실제와 가까워지는 좋은 모델이 된다. 손실 함수는 실제 값과 예측 값 사이의 오차를 모두 더하여 제곱한 이차 함수 형태이다. 따라서 손실 함수가 더 작아지려면 손실 함수의 그래디언트(기울기)가 작아지도록 가중치를 업데이트 해야 한다. 이 때, 그래디언트가 양수라면 가중치를 줄이고 음수라면 가중치를 늘려야 한다. 그런데 시그모이드 함수는 출력값이 항상 양수이므로 그래디언트가 항상 양수가 나오므로 

이에 대한 자세한 내용은 gradident 포스트를 확인해보자.


그래디언트는 손실 함수의 변화율을 나타낸다.
<!--
	•	

    	
    
	•	만약 그래디언트 방향을 따르지 않고 임의의 방향으로 이동하면, 학습 속도가 느려지거나 손실 함수가 더 커질 수 있습니다.


    6. 그래디언트 방향이 잘못될 때

	1.	Zig-Zag 현상:
	•	그래디언트가 항상 같은 방향(양수/음수)으로 편향되면, 가중치가 최적화 경로를 따라가지 못하고 진동하며 비효율적으로 학습됩니다.
	2.	그래디언트 소실:
	•	그래디언트 값이 너무 작으면, 업데이트 방향은 맞더라도 가중치가 거의 움직이지 않아 학습이 멈춥니다.
-->

 Gradient Vanishing 문제가 있어 최근에는 사용하지 않는다. Gradient Vanishing(기울기 소실)은 신경망의 역전파(Backpropagation) 과정에서 발생하는 문제로, 네트워크가 깊어질수록 입력층 근처로 전달되는 그래디언트 값이 점점 작아져 가중치가 제대로 업데이트 되지 않아 학습이 거의 이루어지지 않는 현상을 말한다. 자세한 내용은 Gradient Vanishing 포스트에서 알아보자.

입력값이 매우 크거나 작으면 기울기(그래디언트)가 0에 가까워 학습이 멈출 수 있고, 출력값이 0.5 근처로 몰리는 경우가 많아 학습이 느려질 수 있다. 

#### Tanh 함수 (Hyperbolic Tangent)

<p align="center">
  <img src="https://mlnotebook.github.io/img/transferFunctions/tanh.png" alt="tagn">
</p>

$$\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$$

하이퍼볼릭 탄젠트 함수는 시그모이드와 유사하지만 출력값이 중심을 0으로 가지므로 시그모이드에서 학습이 느려지던 문제를 해결하였다. 하지만 여전히 Gradient Vanishing 문제가 동일하게 발생한다. 

이 밖에도 Softmax, Leakly ReLU, ReLU 등 다양한 활성화 함수가 있다.

<!-- >
2. 출력이 음수라면 가중치가 올바르게 맞춰질까?

출력이 음수인 경우에는  $\sigma(z)$ 가 Zero-Centered(출력값이 양수와 음수로 대칭적)인 함수라면, 가중치 업데이트가 더 균형적으로 이루어질 가능성이 크다.


Tanh 함수를 예로 들면:

\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}

	•	출력 범위:  -1 에서  1 .
	•	출력값이  0 을 중심으로 대칭적이기 때문에, 역전파 과정에서 기울기의 부호가 양수와 음수로 균형 있게 나타납니다.
	•	따라서 가중치가 업데이트될 때 좌우로 요동치지 않고, 더 효율적으로 최적화를 진행합니다.