---
title:  "Shallow Neural Network(얕은 신경망): 딥러닝의 기본"
date:   2024-11-03 10:03:36 +0530
categories: [AI, 딥러닝]
tags:
    [
        SNN,
        인공신경망,
        딥러닝,
		    AI,
    ]
use_math: true 
published: true
---

얕은 신경망(Shallow Neural Network)은 인공 신경망의 가장 기본적인 형태로, 하나의 은닉층(hidden layer)만을 포함하는 신경망을 의미한다.
___

얕은 신경망(Shallow Neural Network)
---

퍼셉트론과 같은 단층 인공신경망으로는 XOR와 같은 비선형 문제는 해결할 수 없다는 것을 알았다. 퍼셉트론과 달리 인공신경망에 __은닉층(hidden layer)__ 을 추가한 Multi Layer Neural Network는 복잡한 패턴의 문제를 해결할 수 있다. 은닉층을 통해 데이터를 비선형적으로 변환할 수 있기 때문이다.

<p align="center">
  <img src="https://www.researchgate.net/profile/Hadley-Brooks/publication/270274130/figure/fig3/AS:667886670594050@1536247999230/Architecture-of-a-multilayer-neural-network-with-one-hidden-layer-The-input-layer.png" alt ="snn">
</p>

Shallow Neural Network는 은닉층이 1개 있는 얕은 신경망으로 

- 입력층(Input Layer): 데이터를 입력받는 층. 각 뉴런이 입력 데이터의 한 특성(feature)을 담당.
- 은닉층(Hidden Layer): 데이터를 가공하고 패턴을 학습하는 층.
- 출력층(Output Layer): 결과를 출력하는 층.

은닉층에 여러 개의 unit(뉴런)을 둘 수 있다. 
출력할 때는 퍼셉트론과 마찬가지로 활성화 함수를 통해서 비선형화해야 한다. 활성화 함수가 없으면 모든 은닉층이 단순한 선형 변환만 하므로, 입력층과 출력층 사이에 아무리 더 많은 층을 추가해도 최종 출력은 여전히 선형이 된다.

예를 들어, 첫 번째 층이 $z_1 = W_1 \cdot x + b_1$,
두 번째 층이 $z_2 = W_2 \cdot z_1 + b_2$ 일 때,

이걸 풀어서 쓰면 $z_2 = W_2 \cdot (W_1 \cdot x + b_1) + b_2 $와 같고

정리하면 $z_2 = (W_2 \cdot W_1) \cdot x + (W_2 \cdot b_1 + b_2)$이 된다.

결국, 여러 층을 거쳐도 $z = W \cdot x + b$ 로 선형이 될 수 밖에 없다.
활성화 함수가 있으면 은닉층이 비선형성을 추가하여 모델이 더욱 복잡한 데이터를 학습할 수 있다.

얕은 신경망 직관적으로 이해하기
---
그렇다면 왜 은닉층이 생기면 더 복잡한 비선형적 문제를 해결할 수 있는 것일까? 이를 좀 더 직관적으로 이해해보자.

입력 $x$ 를 받아서 출력 $y$ 를 생성하는 함수  $y = f[x, \phi]$ 라 하자. 이 함수에는 파라미터 $\phi $ 는 뉴런의 가중치와 편향을 나타내며 학습 과정을 통해서 업데이트된다.

$y = f[x, \phi] = \phi_0 + \phi_1 a[\theta_{10} + \theta_{11}x] + \phi_2 a[\theta_{20} + \theta_{21}x] + \phi_3 a[\theta_{30} + \theta_{31}x]$

주어진 함수를 뜯어보자.

각 입력에 대해서 은닉층에서 다음을 계산한다.
$\theta_{10} + \theta_{11}x $
$\theta_{20} + \theta_{21}x $
$\theta_{30} + \theta_{31}x $

위에서 계산된 값을 활성화 함수  $a[\cdot]$ 에 통과시킨다. 예를 들어, ReLU 함수라면,

$a[z] = \begin{cases}
0 & \text{if } z < 0 \\
z & \text{if } z \geq 0
\end{cases}$

$a[z]$ 는  $z \geq 0$ 일 때는 그대로 반환하고,  $z < 0$ 일 때는 0을 반환한다.

각 활성화 값에  $\phi_1, \phi_2, \phi_3$ 를 곱하고 모두 더한 뒤, $\phi_0$ 를 더해서 최종 출력  $y$ 를 얻는다.

$y = \phi_0 + \phi_1 a[\theta_{10} + \theta_{11}x] + \phi_2 a[\theta_{20} + \theta_{21}x] + \phi_3 a[\theta_{30} + \theta_{31}x]$




<center><img src = "https://blog.kakaocdn.net/dn/dhPukn/btswpYpIT7R/kpd6CekDZkwRkIjY5TKwok/img.png" alt ="snn1">
</center>

이 과정을 정리한 그림이 위와 같다.

위 그림에서 a, b, c에 해당하는 것이 은닉층에서 계산한 것이다. 

그 다음 d, e, f가 계산한 값을 활성화 함수에 통과시킨 모습이다. ReLU 함수에 의해서 임계값 0을 지나는 점은 모두 0이 된다. d, e, f 모두 0이 되는 지점에서 clipped 된 것을 볼 수 있다. (0인 지점에 joint가 생겼다.)
active (활성화) 상태라는 것은 활성화 함수를 통과한 뉴런이 “살아있는 상태”로, 입력 데이터를 처리하고 출력 값을 만들어내는 상태이다. 반면 inactive(비활성화)는 뉴런이 “살아있지 않은 상태”로, 입력 데이터를 처리하지 않고 출력 값이  0 인 상태이다. 따라서 joint 부분은 inactive 상태라 할 수 있다. 그 외의 부분은 active 상태이다.

g, h, i는 d, e, f에 가중치 $\phi$를 곱한다. 그러자 직선의 기울기에 변화가 생겼다. g의 $\phi_0$ 의 기울기는 음수였을 것이다.

최종적으로 j에서 g, h, i를 모두 더한 뒤  $\phi_0$ 까지 더하여 마무리한다.


j 에서 회색으로 표현한 부분을 살펴보자. 회색 구간은 $h_2$ 입장에서 보면 뉴런이 inactive(비활성화) 상태이기 때문에, 이 뉴런의 출력 값은 0이 된다. 반면  $h_1$ 과  $h_3$ 는 active(활성화) 상태이기 때문에  이 뉴런들은 입력 데이터를 처리하여 0이 아닌 값을 출력한다. 결국 $h_1$ 과  $h_3$이 회색 영역에 contribute(기여)한다는 것을 알 수 있다.

<center><img src ="https://blog.kakaocdn.net/dn/1o9Yg/btswcOgUgoY/1s4fI33ypWkpyhkZJItIJ0/img.png" alt ="snn2"></center>


D개의 hidden unit이 있으면 D개의 joints가 생기고 D + 1개의 선형적인 구역이 생긴다. j 그림으로 확인해볼 수 있듯이 3개의 hidden unit이 있으므로 총 4개의 구역이 생겼다. hidden unit이 무한히 많아진다면 데이터와 거의 완벽하게 fit된 모델을 만들 수 있다. 이것이 Universal Approximation 이론이다.

Universal Approximation Theorem 
---
shallow neural networks이 특정 조건 하에서 어떤 연속 함수든 원하는 정확도로 근사할 수 있다는 것을 설명한 이론이다. 단, 활성화 함수가 비선형적이어야 하고, hidden unit이 무한히 많거나 충분히 커야 한다.

이론적으로는 무한히 많은 히든 유닛이 필요할 수 있지만, 실제로는 컴퓨팅 리소스에 한계가 있다. 또한 모델이 과적합(overfitting)될 가능성이 있다. 이 내용은 나중에 다시 자세히 알아보자.

따라서 실용적인 모델은 적절한 히든 유닛 수를 선택하고, 특정 패턴만 학습하도록 제한시킨다.

이에 대한 자세한 내용은 Simon J.D. Prince 의 Deep Learning 책을 참고하면 된다. [링크 참조](https://udlbook.github.io/udlbook/)