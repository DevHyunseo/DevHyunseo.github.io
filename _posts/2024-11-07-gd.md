---
title:  "경사하강법(Gradient Descent): 최적화 알고리즘"
date:   2024-11-07 10:03:36 +0530
categories: /categories/머신러닝/
categories: [AI, 머신러닝]
tags:
    [
        선형회귀,
        AI,
        머신러닝
    ]
use_math: true 
---

경사하강법(Gradient Descent)은 머신러닝 모델이 학습하는 과정에서, 손실 함수(Loss Function)를 최소화하기 위해 점진적으로 최적값에 도달하는 반복적으로 계산하는 방법이다.

---

경사하강법(Gradient Descent : GD)
----

#### 손실 함수(Loss Function)

손실함수는 모델이 실제값에 비해 얼마나 오차가 있는지 측정하는 지표로 대표적으로 평균 제곱 오차 (MSE), 잔차 제곱합 (RSS)가 있다. __머신러닝의 모델의 학습(training) 과정은 손실 함수 즉 오차를 최소화하는 방향으로 진행__ 해야 한다. 학습은 손실 함수를 최소화하여 모델의 예측 성능을 점점 더 향상시키는 과정이라 할 수 있다. 경사하강법은 손실 함수를 최소화하는 최적의 파라미터를 찾는 방법이다.

#### 기울기(Gradient)

기울기는 손실 함수의 변화율을 나타낸다. 손실 함수 $L(\theta)$를 파라미터 $\theta$에 대해 <u>미분하여 기울기</u>를 구할 수 있다.

$$\nabla_\theta L(\theta)$$

이렇게 구한 <u>기울기가 0이 되는 지점이 가장 손실 함수가 작아지는 지점</u>이 될 것이다.
경사하강법은 이 기울기를 사용해, 반복적으로 $\beta_0$와 $\beta_1$ 가중치 값을 조금씩 이동시키켜서 기울기를 따라 천천히 내려가는 방법이다.

![image](https://mblogthumb-phinf.pstatic.net/MjAyMTA5MjRfMTIx/MDAxNjMyNDgwNzE5MzE4.WRYkXNv_E4WZ3mq4srqUfb1ZQN6XSGCjg64JjT-KcQUg.YnQRMlKtbhKcCpHO0_gqWTLq8fNjMIOAA2qknSIyR2Ug.JPEG.chromatic_365/경사하강법.jpg?type=w800)

기울기(Gradient)는 항상 손실 함수가 커지는 방향을 가리킨다. 기울기는 함수값이 증가하는 방향을 가리키기 때문이다.


예를 들어, 2차원 함수 $f(x, y)$의 기울기가 다음과 같을 때,

$$\nabla f(x, y) = \left( \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y} \right)$$

$\frac{\partial f}{\partial x}$는  x가 증가할 때 함수 f 값이 얼마나 변하는지 나타내고,
 $\frac{\partial f}{\partial y}$는 y가 증가할 때 함수 f 값이 얼마나 변하는지 나타낸다.

이때의 $f(x, y)$의 변화량 $\Delta f$는 아래 식으로 근사할 수 있다:

$$\Delta f \approx \nabla f \cdot \Delta \mathbf{r}$$

여기서 $ \nabla f$: 기울기 벡터, $ \Delta \mathbf{r}$: 이동 방향 벡터이다.

이 식에서 $\Delta \mathbf{r}$가 $\nabla f$와 같은 방향이라면 $\Delta f > 0$, 즉 함수 값이 증가한다.
반대로 $\Delta \mathbf{r}$가 $-\nabla f$ 방향(반대 방향)이라면 $\Delta f < 0$, 즉 함수 값이 감소한다.

결론적으로, 기울기 벡터는 함수 값이 증가하는 방향을 가리킨다! 참고 영상 [youtube](https://www.youtube.com/watch?v=MeyIV72Gvpw)

우리의 목표는 손실 함수를 최소하고 싶으므로 손실 함수값이 커지는 방향이 아니라 작아지는 방향으로 가야 하므로, <u>기울기의 반대 방향으로 이동</u>해야 한다. 경사하강법은 기울기의 반대 방향으로 조금씩 이동하면서 함수 값을 작게 만들어가고, 이렇게 해서 최종적으로 최소값에 도달할 수 있다. 처음에 랜덤 초기값을 설정한 후 한번에 조금씩 손실 함수가 감소하는 방향으로 진행하여 알고리즘이 최솟값에 수렴하도록 하여 점진적으로 모델을 향상시킨다. 

### 학습률 (Learning Rate)

#### 학습률

학습률은 손실 함수의 기울기(Gradient)에 곱해지는 값이다.

$$
\theta_{t+1} = \theta_t - \eta \cdot \nabla L(\theta_t)
$$

- $\theta_t$ : 현재 단계에서의 모델 파라미터 값 (선형 회귀에서의 기울기와 절편)

- $\nabla L(\theta_t)$ : 손실 함수 $L(\theta)$의 기울기로 $\nabla L(\theta_t)$는 현재 매개변수 $\theta_t$에서 손실 함수의 변화율(편미분 값)으로 손실 함수 값을 증가시키는 방향을 가리킨다.

- $\eta$ : 학습률(learning rate)로, 파라미터 업데이트 시 기울기 방향으로 <u>얼마나 크게</u> 이동할지를 조정하는 비율이다.

- $\theta_{t+1}$ : 업데이트 후 새로운 매개변수 값

손실함수에 기울기를 곱하여 ($\eta \cdot \nabla L(\theta_t$)  현재 기울기의 반대 방향으로 $\eta$만큼 이동한다. 기울기는 함수 값을 증가시키는 방향을 가리키므로, 함수 값을 줄이기 위해 반대 방향으로 이동해야 한다.


각 단계에서 기울기를 계산해, 현재 파라미터 값 $\theta_t$를 조금씩 수정하여 기울기의 반대 방향으로 이동하며 손실 함수 값을 줄여 나갑니다.

<br>

#### 학습률 설정 

학습률 $\eta$는 경사하강법의 성능을 좌우하는 중요한 하이퍼파라미터이다.

- 학습률이 너무 작으면: 학습 속도가 매우 느려지고, 최솟값에 도달하지 못 할 수 있음.
- 학습률이 너무 크면: 손실 함수에서 최솟값을 지나쳐 발산하게 됨.

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F8NoJ4%2FbtqCLNCGauC%2FFn5uXEIGnTFJrNgQ6pqzak%2Fimg.png)

또한 함수가 항상 이차함수처럼 매끄럽진 않기 때문에 잘못된 최솟값을 찾을 수 있다.

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcQDElY%2Fbtrax7IcdFy%2FLex3dR70l4KzS4ISg33Vyk%2Fimg.png)

![image](https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-08-16-gradient_descent/pic5.png)

일반적으로 학습률은 경험적으로 설정하거나, 학습 중 동적으로 조정하는 방법을 사용한다. 비교적 작은 숫자



### 경사하강법의 유형

1) 배치 경사하강법 (Batch Gradient Descent)

	•	설명: 전체 데이터셋에 대해 손실 함수의 기울기를 계산한 뒤, 한 번에 파라미터를 업데이트합니다.
	•	장점: 기울기의 정확도가 높음.
	•	단점: 데이터셋이 클 경우 계산 비용이 매우 큼.

2) 확률적 경사하강법 (Stochastic Gradient Descent, SGD)

	•	설명: 데이터를 한 샘플씩 사용해 기울기를 계산하고 파라미터를 업데이트합니다.
	•	장점: 계산 속도가 빠르고, 지역 최적값(Local Minimum)을 탈출할 가능성이 있음.
	•	단점: 손실 함수가 진동할 가능성이 높음.

3) 미니배치 경사하강법 (Mini-Batch Gradient Descent)

	•	설명: 전체 데이터셋을 작은 배치(Batch)로 나누어 각 배치에 대해 기울기를 계산합니다.
	•	장점: 배치와 SGD의 장점을 결합하여 계산 효율성과 안정성을 모두 제공.
	•	단점: 배치 크기(Batch Size) 선택이 중요함.


### 경사하강법의 변형 및 개선 방법

1) 모멘텀 (Momentum)

	•	이전 단계의 기울기를 참고해 업데이트 속도를 높이고 진동을 줄이는 방법.
	•	파라미터 업데이트 식:

v_t = \gamma v_{t-1} + \eta \nabla_\theta L(\theta)


\theta = \theta - v_t

	•	$\gamma$: 모멘텀 계수 (0.9로 설정하는 경우가 많음)

2) AdaGrad

	•	학습률을 각 파라미터의 변화에 따라 조정하여 학습이 빠르게 진행되도록 하는 방법.
	•	학습률 업데이트 식:

\eta_t = \frac{\eta}{\sqrt{G_{t, i} + \epsilon}}

	•	$G_{t, i}$: 이전 기울기의 제곱 합
	•	$\epsilon$: 작은 값 (0으로 나누는 오류 방지)

3) RMSProp

	•	AdaGrad의 문제점(학습률이 점점 작아짐)을 해결하기 위해, 기울기의 제곱 평균을 계산해 학습률을 조정합니다.

4) Adam

	•	모멘텀과 RMSProp을 결합한 방법으로, 현재 가장 널리 사용되는 최적화 알고리즘.
	•	파라미터 업데이트 식:

m_t = \beta_1 m_{t-1} + (1-\beta_1) \nabla_\theta L(\theta)


v_t = \beta_2 v_{t-1} + (1-\beta_2) \nabla_\theta^2 L(\theta)


\theta = \theta - \eta \cdot \frac{m_t}{\sqrt{v_t} + \epsilon}




    -------







